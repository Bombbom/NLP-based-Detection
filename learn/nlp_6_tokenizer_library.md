# 1. The tokenizers Library

- [1. The tokenizers Library](#1-the-tokenizers-library)
  - [1.1. Introduction](#11-introduction)
  - [1.2. Training a new tokenizer from an old one](#12-training-a-new-tokenizer-from-an-old-one)
  - [1.3. Fast tokenizers' special powers](#13-fast-tokenizers-special-powers)
  - [1.4. Fast tokenizers in the QA pipeline](#14-fast-tokenizers-in-the-qa-pipeline)
  - [1.5. Normalization and pre-tokenization](#15-normalization-and-pre-tokenization)
  - [1.6. Byte-Pair Encoding tokenization](#16-byte-pair-encoding-tokenization)
  - [1.7. WordPiece tokenization](#17-wordpiece-tokenization)
  - [1.8. Unigram tokenization](#18-unigram-tokenization)
  - [1.9. Building a tokenizer, block by block](#19-building-a-tokenizer-block-by-block)

## 1.1. Introduction 
- Be able to train a new tokenizer using an old one as a template
- Understand how to use offsets to map tokensâ€™ positions to their original span of text
- Know the differences between BPE, WordPiece, and Unigram
- Be able to mix and match the blocks provided by the ðŸ¤— Tokenizers library to build your own tokenizer
- Be able to use that tokenizer inside the ðŸ¤— Transformers library
## 1.2. Training a new tokenizer from an old one

## 1.3. Fast tokenizers' special powers

## 1.4. Fast tokenizers in the QA pipeline

## 1.5. Normalization and pre-tokenization

## 1.6. Byte-Pair Encoding tokenization

## 1.7. WordPiece tokenization

## 1.8. Unigram tokenization

## 1.9. Building a tokenizer, block by block

