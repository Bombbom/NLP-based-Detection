{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bombbom/Documents/NLP_in_Detection_System/env_nlp/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "## Step 1.1: Load new tokenizer from pretrained\n",
    "from transformers import AutoTokenizer, TrainingArguments, Trainer\n",
    "import pandas as pd\n",
    "import re\n",
    "from datasets import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "new_tokenizer = AutoTokenizer.from_pretrained(\"/home/bombbom/Documents/NLP_in_Detection_System/save_model/tokenizer/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 2: Load and process dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_pickle(\"/home/bombbom/Documents/NLP_in_Detection_System/dataset_example/labeled_SBW_datasets.pkl\")\n",
    "data = data[['source_code']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_comments(string):\n",
    "    pattern = r\"(\\\".*?\\\"|\\'.*?\\')|(/\\*.*?\\*/|//[^\\r\\n]*$)\"\n",
    "    # first group captures quoted strings (double or single)\n",
    "    # second group captures comments (//single-line or /* multi-line */)\n",
    "    regex = re.compile(pattern, re.MULTILINE|re.DOTALL)\n",
    "    def _replacer(match):\n",
    "        # if the 2nd group is not None, then we have captured a real comment string.\n",
    "        if match.group(2) is not None:\n",
    "            return \"\" \n",
    "        else: # otherwise, we will return the 1st group\n",
    "            return match.group(1) \n",
    "    return regex.sub(_replacer, string)\n",
    "\n",
    "operators3 = {'<<=', '>>='}\n",
    "operators2 = {\n",
    "    '->', '++', '--',\n",
    "    '!~', '<<', '>>', '<=', '>=',\n",
    "    '==', '!=', '&&', '||', '+=',\n",
    "    '-=', '*=', '/=', '%=', '&=', '^=', '|='\n",
    "}\n",
    "operators1 = {\n",
    "    '(', ')', '[', ']', '.',\n",
    "    '+', '-', '*', '&', '/',\n",
    "    '%', '<', '>', '^', '|',\n",
    "    '=', ',', '?', ':', ';',\n",
    "    '{', '}'\n",
    "}\n",
    "def tokenize(line):\n",
    "        line = line.replace(\"\\n\",\" \").replace(\"\\r\\n\",\" \").replace(\"\\r\",\" \")\n",
    "        tmp, w = [], []\n",
    "        i = 0\n",
    "        while i < len(line):\n",
    "            # Ignore spaces and combine previously collected chars to form words\n",
    "            if line[i] == ' ':\n",
    "                tmp.append(''.join(w))\n",
    "                tmp.append(line[i])\n",
    "                w = []\n",
    "                i += 1\n",
    "            # Check operators and append to final list\n",
    "            elif line[i:i + 3] in operators3:\n",
    "                tmp.append(''.join(w))\n",
    "                tmp.append(line[i:i + 3])\n",
    "                w = []\n",
    "                i += 3\n",
    "            elif line[i:i + 2] in operators2:\n",
    "                tmp.append(''.join(w))\n",
    "                tmp.append(line[i:i + 2])\n",
    "                w = []\n",
    "                i += 2\n",
    "            elif line[i] in operators1:\n",
    "                tmp.append(''.join(w))\n",
    "                tmp.append(line[i])\n",
    "                w = []\n",
    "                i += 1\n",
    "            # Character appended to word list\n",
    "            else:\n",
    "                w.append(line[i])\n",
    "                i += 1\n",
    "        # Filter out irrelevant strings\n",
    "        res = list(filter(lambda c: c != '', tmp))\n",
    "        return ' '.join(list(filter(lambda c: c != ' ', res)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.source_code = data.source_code.apply(remove_comments)\n",
    "data.source_code = data.source_code.apply(tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(data,test_size=0.3, random_state=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val = train_test_split(train,test_size=0.15, random_state=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "del data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = Dataset.from_pandas(train)\n",
    "data_test = Dataset.from_pandas(test)\n",
    "data_val = Dataset.from_pandas(val)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    return new_tokenizer([\" \".join(x) for x in examples[\"source_code\"]], padding = \"max_length\", truncation=True)\n",
    "\n",
    "block_size = 128\n",
    "\n",
    "def group_texts(examples):\n",
    "    # Concatenate all texts.\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
    "    # customize this part to your needs.\n",
    "    if total_length >= block_size:\n",
    "        total_length = (total_length // block_size) * block_size\n",
    "    # Split by chunks of block_size.\n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Prepare dataset for MLM task "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               \r"
     ]
    }
   ],
   "source": [
    "tokenized_data_train = data_train.map(preprocess_function, batched = True, num_proc=4, remove_columns = data_train.column_names)\n",
    "tokenized_data_val = data_val.map(preprocess_function, batched = True, num_proc=4, remove_columns = data_val.column_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "lm_dataset_train = tokenized_data_train.map(group_texts, batched=True, num_proc=4 )\n",
    "lm_dataset_val = tokenized_data_val.map(group_texts, batched=True, num_proc=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 112644\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_dataset_train"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Load pretrained model distilbert-base-uncased\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DistilBertForMaskedLM(\n",
      "  (activation): GELUActivation()\n",
      "  (distilbert): DistilBertModel(\n",
      "    (embeddings): Embeddings(\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (transformer): Transformer(\n",
      "      (layer): ModuleList(\n",
      "        (0-5): 6 x TransformerBlock(\n",
      "          (attention): MultiHeadSelfAttention(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (ffn): FFN(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (activation): GELUActivation()\n",
      "          )\n",
      "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (vocab_transform): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (vocab_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "  (vocab_projector): Linear(in_features=768, out_features=30522, bias=True)\n",
      "  (mlm_loss_fct): CrossEntropyLoss()\n",
      ")\n",
      "DistilBertForMaskedLM(\n",
      "  (activation): GELUActivation()\n",
      "  (distilbert): DistilBertModel(\n",
      "    (embeddings): Embeddings(\n",
      "      (word_embeddings): Embedding(52000, 768)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (transformer): Transformer(\n",
      "      (layer): ModuleList(\n",
      "        (0-5): 6 x TransformerBlock(\n",
      "          (attention): MultiHeadSelfAttention(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (ffn): FFN(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (activation): GELUActivation()\n",
      "          )\n",
      "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (vocab_transform): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (vocab_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "  (vocab_projector): Linear(in_features=768, out_features=52000, bias=True)\n",
      "  (mlm_loss_fct): CrossEntropyLoss()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=new_tokenizer, mlm_probability=0.15)\n",
    "from transformers import AutoModelForMaskedLM\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"distilbert-base-uncased\")\n",
    "print(model)\n",
    "# %%\n",
    "len(new_tokenizer)\n",
    "## Resize \n",
    "model.resize_token_embeddings(len(new_tokenizer))\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Step 5: Setup training_args \n",
    "from transformers import Trainer, TrainingArguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"pre-train-mlm\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.01,\n",
    "    save_steps=10000,\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=lm_dataset_train,\n",
    "    eval_dataset=lm_dataset_val,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "\n",
    "## Step 6: Trainning model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()\n",
    "\n",
    "\n",
    "## Step 7: Saving model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"/home/bombbom/Documents/Project_CS_Vul_Detect/py/save_trained_models/pre_train/pre_train_6/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
